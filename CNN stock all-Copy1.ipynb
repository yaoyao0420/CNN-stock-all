{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras import backend as K\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "import sklearn as sk\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler\n",
    "mms = MinMaxScaler()\n",
    "mas = MaxAbsScaler()\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PastDays = 21\n",
    "n_classes = 3\n",
    "FeatureNums = 29\n",
    "batch_size = 32\n",
    "num_epoch = 2\n",
    "\n",
    "n_filters = 32\n",
    "kernel_size = [5, 5]\n",
    "pool_size = [2, 2]\n",
    "\n",
    "img_cols = PastDays-1\n",
    "img_rows = FeatureNums\n",
    "input_shape = (img_cols, img_rows, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(df):\n",
    "    high = df['high']\n",
    "    low = df['low']\n",
    "    change = df['change']\n",
    "    change_array = []\n",
    "    delist_array = []\n",
    "    for i in range(len(df)):\n",
    "        if high[i] != low[i]:\n",
    "            delist_array.append(1)\n",
    "            if change[i] > 10.0:\n",
    "                change_array.append(10.0)\n",
    "            elif change[i] < -10.0:\n",
    "                change_array.append(-10.0)\n",
    "            else:\n",
    "                change_array.append(change[i])\n",
    "        else:\n",
    "            delist_array.append(0)\n",
    "    df['change_update'] = pd.DataFrame(change_array)\n",
    "    df['delist'] = pd.DataFrame(delist_array)\n",
    "    \n",
    "    # 去除当日停牌数据\n",
    "    df = df[df['delist'].isin([1])]\n",
    "    return df[['open', 'high', 'low', 'close', 'volume', 'amount', 'change_update', 'EMA12', 'EMA26', \n",
    "                'MACD', 'MACDsignal', 'MACDhist', 'RSI', 'ROC', 'k', 'd', 'j', 'ROLL_UP',\n",
    "                'ROLL_MB', 'ROLL_DN', 'diff1', 'diff2', 'OBV', 'm1_yoy', 'm2_yoy', 'dd', 'fix3', 'fix6', 'erd']], df[['date', 'code', 'change_update']]\n",
    "\n",
    "def DataProcess(df, code):\n",
    "    data = df\n",
    "    code = code\n",
    "    label = df['change_update'].values\n",
    "\n",
    "    mas.fit(data)\n",
    "    data = mas.transform(data)                                                                           \n",
    "    \n",
    "    label_array = []\n",
    "    for i in range(len(label)):\n",
    "        # 平1，跌0、涨2，股票收盘价波动不超过+-0.5%时标签为平\n",
    "        if label[i] >= 0.5:\n",
    "            label_array.append(2)\n",
    "        elif label[i] <= -0.5:\n",
    "            label_array.append(0)\n",
    "        else:\n",
    "            label_array.append(1)\n",
    "            \n",
    "    data = pd.DataFrame(data)\n",
    "    \n",
    "    return data, label_array, code\n",
    "\n",
    "def win(data, label, PastDays):\n",
    "    # 设定滑动窗口大小为1\n",
    "    newt = []\n",
    "    newtr = []\n",
    "    newtl = []\n",
    "    newl = []\n",
    "    for i in range(len(data)-PastDays+1):\n",
    "        for j in range(PastDays):\n",
    "            if(j < PastDays-1):                \n",
    "                newt.append(data[i+j])\n",
    "            else:\n",
    "                newtl.append(label[i+j-1])\n",
    "                    \n",
    "                newtr.append(newt)\n",
    "                newt = []\n",
    "    dataset = np.array(newtr)\n",
    "    labelset = np.array(newtl)\n",
    "    \n",
    "    return dataset, labelset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_loss_fig(model_log):\n",
    "    fig = plt.figure()\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(model_log.history['acc'])\n",
    "    plt.plot(model_log.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(model_log.history['loss'])\n",
    "    plt.plot(model_log.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_code = pd.read_csv(\"stock_name.csv\")\n",
    "stock_code = stock_code.values\n",
    "strain_csv = []\n",
    "stest_csv = []\n",
    "for i in range(len(stock_code)):\n",
    "    filepath_train = '2018strain/'\n",
    "    stock_train = filepath_train+stock_code[i]\n",
    "    filepath_test = '2018stest/'\n",
    "    stock_test = filepath_test+stock_code[i]\n",
    "    strain_csv.append(stock_train)\n",
    "    stest_csv.append(stock_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pre_process(csv):\n",
    "    dataset = []\n",
    "    labelset = []\n",
    "    for stock_item in csv:\n",
    "        df = pd.read_csv(stock_item[0])\n",
    "        close = df['close']\n",
    "\n",
    "        # 拆分训练数据与测试数据并获取标签\n",
    "        data_num = df.shape[0]\n",
    "        data, code = getData(df)\n",
    "        data = data.dropna(axis=0)\n",
    "        code = code.dropna(axis=0)\n",
    "        data_1, label_1, code_1 = DataProcess(data, code)\n",
    "        data_1 = data_1.dropna(axis=0)\n",
    "        code_1 = code_1.dropna(axis=0)\n",
    "        data_values = data_1.values\n",
    "\n",
    "        data_win, label_win = win(data_values, label_1, PastDays)\n",
    "\n",
    "        num = len(data_win)\n",
    "        data_random_num = list(range(0, num))\n",
    "        random.shuffle(data_random_num)\n",
    "\n",
    "        data_random = []\n",
    "        label_random = []\n",
    "        for item in data_random_num:\n",
    "            data_random.append(data_win[item])\n",
    "            label_random.append(label_win[item])\n",
    "\n",
    "        labelset_01 = np_utils.to_categorical(label_random, n_classes)\n",
    "        \n",
    "        dataset.extend(data_random)\n",
    "        labelset.extend(labelset_01)\n",
    "        \n",
    "    print(\"FINISHED\")\n",
    "    return dataset, labelset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pre_process(stock_item):\n",
    "    df = pd.read_csv(stock_item)\n",
    "\n",
    "    data_num = df.shape[0]\n",
    "    data, code = getData(df)\n",
    "    data = data.dropna(axis=0)\n",
    "    code = code.dropna(axis=0)\n",
    "    data_1, label_1, code_1 = DataProcess(data, code)\n",
    "    data_1 = data_1.dropna(axis=0)\n",
    "    code_1 = code_1.dropna(axis=0)\n",
    "    data_values = data_1.values\n",
    "\n",
    "    data_win, label_win = win(data_values, label_1, PastDays)\n",
    "\n",
    "    num = len(data_win)\n",
    "    data_random_num = list(range(0, num))\n",
    "    random.shuffle(data_random_num)\n",
    "\n",
    "    data_random = []\n",
    "    label_random = []\n",
    "    for item in data_random_num:\n",
    "        data_random.append(data_win[item])\n",
    "        label_random.append(label_win[item])\n",
    "\n",
    "    labelset_01 = np_utils.to_categorical(label_random, n_classes)\n",
    "    code_1 = pd.DataFrame(code_1)\n",
    "    print(\"FINISHED\")\n",
    "    return data_random, labelset_01, code_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 之前生成训练数据和训练标签的代码，现在可以不使用\n",
    "train_data, train_label = data_pre_process(strain_csv)\n",
    "train_data = np.array(train_data)\n",
    "train_data = train_data.reshape(len(train_data), img_cols, img_rows, 1)\n",
    "train_data = train_data.astype('float32')\n",
    "train_label = np.array(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = []\n",
    "def model(stock_name, train_data, test_data, train_label, test_label):\n",
    "    test_data = np.array(test_data)\n",
    "    X_train = train_data\n",
    "    X_test  = test_data.reshape(len(test_data), img_cols, img_rows, 1)\n",
    "\n",
    "    X_test = X_test.astype('float32')\n",
    "    \n",
    "    y_train = train_label\n",
    "    y_test = np.array(test_label)\n",
    "\n",
    "    print(X_train.shape, y_train.shape)\n",
    "    print(X_test.shape, y_test.shape)\n",
    "    \n",
    "    if X_train.shape[0] == 0:\n",
    "        return \n",
    "    else:\n",
    "        # 2D model\n",
    "        model = Sequential()\n",
    "        model.add(Convolution2D(filters=6, kernel_size=(5,5), padding='valid', input_shape=input_shape, activation='tanh'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(Convolution2D(filters=16, kernel_size=(5,5), padding='valid', activation='tanh'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(120, activation='tanh'))\n",
    "        model.add(Dense(84, activation='tanh'))\n",
    "        model.add(Dense(n_classes, activation='softmax'))\n",
    "        sgd = SGD(lr=0.05, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        # 保存模型的权重\n",
    "        # checkpoint best neural network model only\n",
    "        filepath = stock_name + \".CNN.weight.best.h5\"\n",
    "        path.append(filepath)\n",
    "        # checkpoint\n",
    "        checkpoint = ModelCheckpoint(filepath,\n",
    "                                                     monitor='val_acc',\n",
    "                                                     verbose=1,\n",
    "                                                     save_best_only=True,\n",
    "                                                     mode='max')\n",
    "\n",
    "        model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                             optimizer=keras.optimizers.Adadelta(),\n",
    "                             metrics=['accuracy'])\n",
    "\n",
    "        model.save_weights(filepath)\n",
    "\n",
    "        model_log = model.fit(X_train, y_train,\n",
    "                                       batch_size=batch_size,\n",
    "                                       epochs=num_epoch,\n",
    "                                       verbose=1,\n",
    "                                       validation_data=(X_test, y_test),\n",
    "                                       callbacks=[checkpoint])\n",
    "        acc_loss_fig(model_log)\n",
    "    \n",
    "        model.load_weights(filepath)\n",
    "        y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "        y_pred_01 = np_utils.to_categorical(y_pred, n_classes)\n",
    "        y_true = y_test\n",
    "        F1_value = f1_score(y_true, y_pred_01, average='weighted')\n",
    "        conf_matrix = sk.metrics.confusion_matrix(y_true.argmax(axis=1), y_pred_01.argmax(axis=1))\n",
    "        score = model.evaluate(X_test, y_test, verbose=0)\n",
    "        print(\"F1_score = \", F1_value)\n",
    "        print(\"confusion matrix\\n\", conf_matrix)\n",
    "        print(\"Test Score = \", score[0])\n",
    "        print(\"Test Accuracy = \", score[1])\n",
    "        return F1_value, conf_matrix, score[0], score[1], y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000002\n",
      "FINISHED\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'bytearray' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f278cb0efcaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstock_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_pre_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstock_item\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mf1_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstock_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;31m# 生产保存每个股票的预测情况trend/code.csv文件，日期、code、预测涨跌、真实涨跌4个字段\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'code'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'real_change'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pred_change'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#创建一个空的dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-fe23b2f5593e>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(stock_name, train_data, test_data, train_label, test_label)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'bytearray' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "def write_csv(data, stock_name):\n",
    "    file_name = 'trend/'+ stock_name + 'code.csv'\n",
    "    try:\n",
    "        data.to_csv(file_name)\n",
    "    except UnicodeEncodeError:\n",
    "        print(\"ERROR\")\n",
    "        \n",
    "stock_code = []\n",
    "F1 = []\n",
    "Test_Score = []\n",
    "Test_Accuracy = []\n",
    "for stock_item in stest_csv:\n",
    "    stock_name = stock_item[0][10: 16]\n",
    "    print(stock_name)\n",
    "    test_data, test_label, test_code = test_pre_process(stock_item[0])\n",
    "    f1_value, conf, ts, ta, y_pred = model(stock_name, train_data, test_data, train_label, test_label)\n",
    "    # 生产保存每个股票的预测情况trend/code.csv文件，日期、code、预测涨跌、真实涨跌4个字段\n",
    "    code = pd.DataFrame(columns = ['date', 'code', 'real_change', 'pred_change']) #创建一个空的dataframe\n",
    "    code['date'] = test_code['date']\n",
    "    code['code'] = test_code['code']\n",
    "    code['pred_change'] = pd.DataFrame(y_pred)\n",
    "    code['real_change'] = pd.DataFrame(test_label)\n",
    "    write_csv(code, stock_name)\n",
    "    \n",
    "    # 存储每支股票的code，F1，accuracy和score\n",
    "    path.append(filepath)\n",
    "    stock_code.append(stock_name)\n",
    "    F1.append(f1_value)\n",
    "    Test_Score.append(ts)\n",
    "    Test_Accuracy.append(ta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存储所有股票的F1、accuracy、score，存入统计文件score.csv中，有code, F1, score三列\n",
    "score = pd.DataFrame(columns = ['code', 'F1', 'Test Score', 'Test Accuracy']) \n",
    "score['code'] = pd.DataFrame(stock_code)\n",
    "score['F1'] = pd.DataFrame(F1)\n",
    "score['Test Score'] = pd.DataFrame(Test_Score)\n",
    "score['Test Accuracy'] = pd.DataFrame(Test_Accuracy)\n",
    "score.to_csv('trend/score.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os.path\n",
    "\n",
    "# 因为train_data文件过大，超过2G，所以需要用如下方法来存储到pickle中，但通过此方法存储后train data的形式改变了\n",
    "file_path = \"train_data.pkl\"\n",
    "n_bytes = 2**31\n",
    "max_bytes = 2**31 - 1\n",
    "train_data = bytearray(n_bytes)\n",
    "# write\n",
    "bytes_out = pickle.dumps(train_data)\n",
    "with open(file_path, 'wb') as f_out:\n",
    "    for idx in range(0, len(bytes_out), max_bytes):\n",
    "        f_out.write(bytes_out[idx:idx+max_bytes])\n",
    "        \n",
    "with open(r'train_label.pkl', 'wb') as f:\n",
    "    pickle.dump(train_label, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#打开文件\n",
    "pickle_open = open('train_data.pkl','rb')\n",
    "train_data = pickle.load(pickle_open)   #重新导入数据\n",
    "\n",
    "pickle_open = open('train_label.pkl','rb')\n",
    "train_label = pickle.load(pickle_open)   #重新导入数据"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
